{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Basic Custom Training Container</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to build and use a basic custom Docker container for training with Amazon SageMaker. Reference documentation is available at https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining some variables like the current execution role, the ECR repository that we are going to use for pushing the custom Docker container and a default Amazon S3 bucket to be used by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230755935769\n",
      "us-west-2\n",
      "arn:aws:iam::230755935769:role/SageMakerExecutionRoleMLOps\n",
      "sagemaker-us-west-2-230755935769\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "\n",
    "ecr_repository_name = 'matterport_mask_rcnn'\n",
    "role = get_execution_role()\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(account_id)\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the Dockerfile which defines the statements for building our custom SageMaker training container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:1.15.4-gpu-py36-cu100-ubuntu18.04\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mRUN\u001b[39;49;00m apt update\r\n",
      "\u001b[34mRUN\u001b[39;49;00m apt install -y python3-opencv\r\n",
      "\r\n",
      "\u001b[34mRUN\u001b[39;49;00m git clone https://github.com/catwhiskers/Mask_RCNN.git && \u001b[36mcd\u001b[39;49;00m Mask_RCNN && pip install -r requirements.txt && python setup.py install  \r\n"
     ]
    }
   ],
   "source": [
    "! pygmentize Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At high-level the Dockerfile specifies the following operations for building this container:\n",
    "<ul>\n",
    "    <li>Start from Ubuntu 16.04</li>\n",
    "    <li>Define some variables to be used at build time to install Python 3</li>\n",
    "    <li>Some handful libraries are installed with apt-get</li>\n",
    "    <li>We then install Python 3 and create a symbolic link</li>\n",
    "    <li>We install some Python libraries like numpy, pandas, ScikitLearn, etc.</li>\n",
    "    <li>We set e few environment variables, including PYTHONUNBUFFERED which is used to avoid buffering Python standard output (useful for logging)</li>\n",
    "    <li>Finally, we copy all contents in <strong>code/</strong> (which is where our training code is) to the WORKDIR and define the ENTRYPOINT</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Build and push the container</h3>\n",
    "We are now ready to build this container and push it to Amazon ECR. This task is executed using a shell script stored in the ../script/ folder. Let's take a look at this script and then execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/usr/bin/env bash\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# This script shows how to build the Docker image and push it to ECR to be ready for use\u001b[39;49;00m\r\n",
      "\u001b[37m# by SageMaker.\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# The argument to this script is the image name. This will be used as the image on the local\u001b[39;49;00m\r\n",
      "\u001b[37m# machine and combined with the account and region to form the repository name for ECR.\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[31mDIR\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[34m$(\u001b[39;49;00m \u001b[36mcd\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[34m$(\u001b[39;49;00m dirname \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mBASH_SOURCE\u001b[39;49;00m[0]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m && \u001b[36mpwd\u001b[39;49;00m \u001b[34m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[36msource\u001b[39;49;00m \u001b[31m$DIR\u001b[39;49;00m/set_env.sh\r\n",
      "\r\n",
      "\u001b[37m# set region\u001b[39;49;00m\r\n",
      "\u001b[31mregion\u001b[39;49;00m=\r\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[33m\"\u001b[39;49;00m\u001b[31m$#\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m -eq \u001b[34m1\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\r\n",
      "    \u001b[31mregion\u001b[39;49;00m=\u001b[31m$1\u001b[39;49;00m\r\n",
      "\u001b[34melse\u001b[39;49;00m\r\n",
      "    \u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33musage: \u001b[39;49;00m\u001b[31m$0\u001b[39;49;00m\u001b[33m <aws-region>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[36mexit\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\r\n",
      "\u001b[34mfi\u001b[39;49;00m\r\n",
      "  \r\n",
      "\r\n",
      "\u001b[31mimage\u001b[39;49;00m=\u001b[31m$IMAGE_NAME\u001b[39;49;00m\r\n",
      "\u001b[31mtag\u001b[39;49;00m=\u001b[31m$IMAGE_TAG\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Get the account number associated with the current IAM credentials\u001b[39;49;00m\r\n",
      "\u001b[31maccount\u001b[39;49;00m=\u001b[34m$(\u001b[39;49;00maws sts get-caller-identity --query Account --output text\u001b[34m)\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -ne \u001b[34m0\u001b[39;49;00m ]\r\n",
      "\u001b[34mthen\u001b[39;49;00m\r\n",
      "    \u001b[36mexit\u001b[39;49;00m \u001b[34m255\u001b[39;49;00m\r\n",
      "\u001b[34mfi\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[31mfullname\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31maccount\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.dkr.ecr.\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.amazonaws.com/\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mtag\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# If the repository doesn't exist in ECR, create it.\u001b[39;49;00m\r\n",
      "aws ecr describe-repositories --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --repository-names \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m > /dev/null \u001b[34m2\u001b[39;49;00m>&\u001b[34m1\u001b[39;49;00m\r\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -ne \u001b[34m0\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\r\n",
      "    aws ecr create-repository --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --repository-name \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m > /dev/null\r\n",
      "\u001b[34mfi\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# Build the docker image locally with the image name and then push it to ECR\u001b[39;49;00m\r\n",
      "\u001b[37m# with the full name.\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Get the login command from ECR and execute it directly\u001b[39;49;00m\r\n",
      "\u001b[34m$(\u001b[39;49;00maws ecr get-login --no-include-email --region us-west-2  --registry-ids \u001b[34m763104351884\u001b[39;49;00m\u001b[34m)\u001b[39;49;00m\r\n",
      "\r\n",
      "docker build .  -t \u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m \r\n",
      "docker tag \u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m \u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Get the login command from ECR and execute it directly\u001b[39;49;00m\r\n",
      "\u001b[34m$(\u001b[39;49;00maws ecr get-login --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --no-include-email\u001b[34m)\u001b[39;49;00m\r\n",
      "docker push \u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\r\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -eq \u001b[34m0\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\r\n",
      "\t\u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mAmazon ECR URI: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[34melse\u001b[39;49;00m\r\n",
      "\t\u001b[36mecho\u001b[39;49;00m \u001b[33m\"Error: Image build and push failed\"\u001b[39;49;00m\r\n",
      "\t\u001b[36mexit\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\r\n",
      "\u001b[34mfi\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "! pygmentize build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>--------------------------------------------------------------------------------------------------------------------</h3>\n",
    "\n",
    "The script builds the Docker container, then creates the repository if it does not exist, and finally pushes the container to the ECR repository. The build task requires a few minutes to be executed the first time, then Docker caches build outputs to be reused for the subsequent build operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  482.4MB\n",
      "Step 1/4 : FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:1.15.4-gpu-py36-cu100-ubuntu18.04\n",
      " ---> 990ca849a51a\n",
      "Step 2/4 : RUN apt update\n",
      " ---> Using cache\n",
      " ---> 0d3b66d75207\n",
      "Step 3/4 : RUN apt install -y python3-opencv\n",
      " ---> Using cache\n",
      " ---> 933f627f3802\n",
      "Step 4/4 : RUN git clone https://github.com/catwhiskers/Mask_RCNN.git && cd Mask_RCNN && pip install -r requirements.txt && python setup.py install\n",
      " ---> Using cache\n",
      " ---> 812b58a06c66\n",
      "Successfully built 812b58a06c66\n",
      "Successfully tagged matterport_mask_crnn:latest\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "The push refers to repository [230755935769.dkr.ecr.us-west-2.amazonaws.com/matterport_mask_crnn]\n",
      "\n",
      "\u001b[1B19808a1a: Preparing \n",
      "\u001b[1Bc07844ef: Preparing \n",
      "\u001b[1B20cc965c: Preparing \n",
      "\u001b[1Bf231bfcd: Preparing \n",
      "\u001b[1Bb2438270: Preparing \n",
      "\u001b[1B6685c833: Preparing \n",
      "\u001b[1Bb7d25199: Preparing \n",
      "\u001b[1B58609659: Preparing \n",
      "\u001b[1Bded9f82f: Preparing \n",
      "\u001b[1B65795983: Preparing \n",
      "\u001b[1B4be7da4e: Preparing \n",
      "\u001b[1B43cf7a5d: Preparing \n",
      "\u001b[1B6df08514: Preparing \n",
      "\u001b[1Bd8ed5776: Preparing \n",
      "\u001b[1Bd8610875: Preparing \n",
      "\u001b[1Bc64e2450: Preparing \n",
      "\u001b[1B3c3f3887: Preparing \n",
      "\u001b[1B44ca32c9: Preparing \n",
      "\u001b[1B85daa06b: Preparing \n",
      "\u001b[1B10e5c00f: Preparing \n",
      "\u001b[1B57a7a8d5: Preparing \n",
      "\u001b[1B488e54e7: Preparing \n",
      "\u001b[1Bd0b5ace3: Preparing \n",
      "\u001b[1B03eff8ce: Preparing \n",
      "\u001b[1Bc7820671: Preparing \n",
      "\u001b[4Bd0b5ace3: Waiting g \n",
      "\u001b[1Bdf553184: Preparing \n",
      "\u001b[2Bdf553184: Layer already exists \u001b[23A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2Klatest: digest: sha256:6c790280dcb1967178bd1a78d47162e110e3bf0d9b0c36f0ebacd0983e1e6d95 size: 6191\n",
      "Amazon ECR URI: 230755935769.dkr.ecr.us-west-2.amazonaws.com/matterport_mask_crnn:latest\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! ./build_and_push.sh us-west-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training with Amazon SageMaker</h3>\n",
    "\n",
    "Once we have correctly pushed our container to Amazon ECR, we are ready to start training with Amazon SageMaker, which requires the ECR path to the Docker container used for training as parameter for starting a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230755935769.dkr.ecr.us-west-2.amazonaws.com/matterport_mask_rcnn:latest\n"
     ]
    }
   ],
   "source": [
    "container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print(container_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the purpose of this example is explaining how to build custom containers, we are not going to train a real model. The script that will be executed does not define a specific training logic; it just outputs the configurations injected by SageMaker and implements a dummy training loop. Training data is also dummy. Let's analyze the code first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize ../docker/code/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload some dummy data to Amazon S3, in order to define our S3-based training channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"val1, val2, val3\" > dummy.csv\n",
    "print(sagemaker_session.upload_data('dummy.csv', bucket, prefix + '/train'))\n",
    "print(sagemaker_session.upload_data('dummy.csv', bucket, prefix + '/val'))\n",
    "! rm dummy.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can execute the training job by calling the fit() method of the generic Estimator object defined in the Amazon SageMaker Python SDK (https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/estimator.py). This corresponds to calling the CreateTrainingJob() API (https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "est = sagemaker.estimator.Estimator(container_image_uri,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='local', # use local mode\n",
    "                                    #train_instance_type='ml.m5.xlarge',\n",
    "                                    base_job_name=prefix)\n",
    "\n",
    "est.set_hyperparameters(hp1='value1',\n",
    "                        hp2=300,\n",
    "                        hp3=0.001)\n",
    "\n",
    "train_config = sagemaker.session.s3_input('s3://{0}/{1}/train/'.format(bucket, prefix), content_type='text/csv')\n",
    "val_config = sagemaker.session.s3_input('s3://{0}/{1}/val/'.format(bucket, prefix), content_type='text/csv')\n",
    "\n",
    "est.fit({'train': train_config, 'validation': val_config })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
